{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set tokenizers parallelism environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Check GPU availability and print info\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Available GPU count: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i} name:\", torch.cuda.get_device_name(i))\n",
    "        print(f\"GPU {i} memory:\", torch.cuda.get_device_properties(i).total_memory / 1e9, \"GB\")\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "test_df = test_df[['id', 'text']].dropna()\n",
    "print(f\"Test data loaded, {len(test_df)} records total\")\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model_path = \"/kaggle/input/deberta_final/pytorch/default/1/deberta_model_persuade_V2\"  # Make sure this is your model save path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Detect available device and move model to that device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded and moved to device: {device}\")\n",
    "\n",
    "# Define tokenize function\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=384)\n",
    "\n",
    "# Process test data\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(tokenize_fn, batched=True, num_proc=1)\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "print(\"Test data processing complete\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Starting prediction generation...\")\n",
    "# Define batch size\n",
    "batch_size = 8\n",
    "all_predictions = []\n",
    "all_probs = []\n",
    "\n",
    "# Batch processing predictions for efficiency\n",
    "for i in range(0, len(test_dataset), batch_size):\n",
    "    batch = test_dataset[i:i+batch_size]\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    # Don't calculate gradients to speed up inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    # Get logits\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Calculate probabilities and predictions\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Move to CPU and convert to numpy\n",
    "    all_predictions.extend(preds.cpu().numpy())\n",
    "    all_probs.extend(probs[:, 1].cpu().numpy())  # Take probability of second class (generated text)\n",
    "    \n",
    "    # Print progress\n",
    "    if (i // batch_size) % 10 == 0:\n",
    "        print(f\"Processed {i+len(batch)}/{len(test_dataset)} records\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_preds = np.array(all_predictions)\n",
    "predicted_probs = np.array(all_probs)\n",
    "\n",
    "# Output detailed results\n",
    "detailed_df = test_df.copy()\n",
    "detailed_df['generated_pred'] = test_preds\n",
    "detailed_df.to_csv(\"/kaggle/working/deberta_detailed_predictions.csv\", index=False)\n",
    "print(\"Detailed prediction results saved\")\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'generated': predicted_probs\n",
    "})\n",
    "submission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
    "print(\"Submission file generated\")\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Number of predicted AI-generated texts: {sum(test_preds)}\")\n",
    "print(f\"Number of predicted human-written texts: {len(test_preds) - sum(test_preds)}\")\n",
    "print(f\"Percentage of predicted AI-generated texts: {sum(test_preds) / len(test_preds) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
