{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5407182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Load human text (from Persuade corpus)\n",
    "print(\"Loading Persuade corpus for human texts...\")\n",
    "persuade_dataset = load_dataset(\"persuade/persuade-corpus\", split=\"train\")\n",
    "persuade_df = pd.DataFrame(persuade_dataset)\n",
    "\n",
    "# Keep only human-written texts (generated == 0)\n",
    "human_df = persuade_df[persuade_df['generated'] == 0].copy()\n",
    "human_df = human_df[['text']].copy()\n",
    "human_df['generated'] = 0\n",
    "\n",
    "print(f\"Number of human texts: {len(human_df)}\")\n",
    "\n",
    "# 2. Generate AI text using SlimPajama dataset as prompts\n",
    "print(\"Loading SlimPajama dataset for prompts...\")\n",
    "slimpajama_dataset = load_dataset(\"cerebras/SlimPajama-627B\", split=\"train\", streaming=True)\n",
    "slimpajama_samples = []\n",
    "\n",
    "# Sample texts from SlimPajama as prompts\n",
    "num_samples = min(5000, len(human_df) * 2)\n",
    "print(f\"Sampling {num_samples} texts from SlimPajama...\")\n",
    "\n",
    "for i, sample in enumerate(slimpajama_dataset):\n",
    "    if i >= num_samples:\n",
    "        break\n",
    "    \n",
    "    text = sample['text']\n",
    "    sentences = text.split('.')\n",
    "    if len(sentences) > 3:\n",
    "        prompt = '.'.join(sentences[:2]) + '.'\n",
    "        slimpajama_samples.append(prompt)\n",
    "    elif len(text) > 10:\n",
    "        prompt = text[:min(100, len(text))]\n",
    "        slimpajama_samples.append(prompt)\n",
    "\n",
    "print(f\"Sampling completed, obtained {len(slimpajama_samples)} text prompts\")\n",
    "\n",
    "# Load non-instruction-tuned models for text generation\n",
    "print(\"Loading LLaMA 2 model...\")\n",
    "llama_model_name = \"meta-llama/Llama-2-7b\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llama_model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "print(\"Loading Mistral model...\")\n",
    "mistral_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_model_name)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(mistral_model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# Generate AI text\n",
    "def generate_completion(prompt, model, tokenizer, max_length=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    completion = generated_text[len(prompt):]\n",
    "    return prompt + completion\n",
    "\n",
    "print(\"Starting AI text generation...\")\n",
    "ai_texts = []\n",
    "for i, prompt in enumerate(tqdm(slimpajama_samples)):\n",
    "    try:\n",
    "        if i % 2 == 0 and i < len(slimpajama_samples) // 2:\n",
    "            # Use LLaMA 2\n",
    "            generated_text = generate_completion(prompt, llama_model, llama_tokenizer)\n",
    "            ai_texts.append({\"text\": generated_text, \"model\": \"llama2\"})\n",
    "        else:\n",
    "            # Use Mistral\n",
    "            generated_text = generate_completion(prompt, mistral_model, mistral_tokenizer)\n",
    "            ai_texts.append({\"text\": generated_text, \"model\": \"mistral\"})\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Generated {i+1}/{len(slimpajama_samples)} AI texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating sample {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Create DataFrame with AI-generated texts\n",
    "ai_df = pd.DataFrame(ai_texts)\n",
    "ai_df = ai_df[['text']].copy()\n",
    "ai_df['generated'] = 1\n",
    "\n",
    "# Ensure AI text count is 10x the human text count\n",
    "if len(ai_df) < 10 * len(human_df):\n",
    "    multiplier = (10 * len(human_df)) // len(ai_df) + 1\n",
    "    ai_df = pd.concat([ai_df] * multiplier)\n",
    "    ai_df = ai_df.sample(n=10 * len(human_df), random_state=42)\n",
    "\n",
    "print(f\"Number of AI-generated texts: {len(ai_df)}\")\n",
    "\n",
    "# 3. Combine datasets and shuffle\n",
    "combined_df = pd.concat([human_df, ai_df], ignore_index=True)\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 4. Save the new training set\n",
    "output_path = 'combined_training_data_large.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nNew training data saved to: {output_path}\")\n",
    "print(f\"Total samples: {len(combined_df)} (Human: {len(human_df)}, AI: {len(ai_df)})\")\n",
    "\n",
    "# 5. Create SlimPajama dataset for the AI models\n",
    "slimpajama_df = pd.DataFrame(ai_texts)\n",
    "slimpajama_df.to_csv('slimpajama.csv', index=False)\n",
    "print(\"SlimPajama dataset saved to: slimpajama.csv\")\n",
    "\n",
    "# 6. Create persuade_combined.csv (the original file with both human and AI texts)\n",
    "# Sample some AI texts to create the AI portion of persuade\n",
    "persuade_ai_count = len(persuade_df[persuade_df['generated'] == 1])\n",
    "persuade_ai_texts = ai_df.sample(n=min(persuade_ai_count, len(ai_df)), random_state=42)\n",
    "\n",
    "# Extend persuade_df with the necessary columns\n",
    "if 'prompt' not in persuade_df.columns:\n",
    "    persuade_df['prompt'] = \"\"\n",
    "if 'model' not in persuade_df.columns:\n",
    "    persuade_df['model'] = \"human\"\n",
    "\n",
    "# Prepare AI portion for persuade\n",
    "persuade_ai_texts['prompt'] = \"\"\n",
    "persuade_ai_texts['model'] = \"ai_generated\"\n",
    "\n",
    "# Select only the columns that match persuade_df\n",
    "persuade_ai_cols = list(set(persuade_df.columns).intersection(set(persuade_ai_texts.columns)))\n",
    "persuade_ai_texts = persuade_ai_texts[persuade_ai_cols]\n",
    "\n",
    "# Add any missing columns in persuade_ai_texts with default values\n",
    "for col in persuade_df.columns:\n",
    "    if col not in persuade_ai_texts.columns:\n",
    "        persuade_ai_texts[col] = None  # Use appropriate default values\n",
    "\n",
    "# Keep only the human portion of the original persuade\n",
    "persuade_human = persuade_df[persuade_df['generated'] == 0].copy()\n",
    "\n",
    "# Combine human and generated portions to create persuade_combined\n",
    "persuade_combined = pd.concat([persuade_human, persuade_ai_texts], ignore_index=True)\n",
    "persuade_combined = persuade_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save persuade_combined\n",
    "persuade_combined.to_csv('persuade_combined.csv', index=False)\n",
    "print(\"Persuade combined dataset saved to: persuade_combined.csv\")\n",
    "\n",
    "# 7. Create test_essays.csv for evaluation\n",
    "test_essays = ai_df.sample(n=min(200, len(ai_df)), random_state=42)\n",
    "test_essays['id'] = [f\"test_{i}\" for i in range(len(test_essays))]\n",
    "test_essays = test_essays[['id', 'text']]\n",
    "test_essays.to_csv('test_essays.csv', index=False)\n",
    "print(\"Test essays file created: test_essays.csv\")\n",
    "\n",
    "print(\"\\nAll datasets have been successfully generated.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
